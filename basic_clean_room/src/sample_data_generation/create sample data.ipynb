{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2214ad07-92b1-4bda-b34f-f31fc758459e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Healthcare Sample Data Generator\n",
    "\n",
    "This notebook generates synthetic healthcare data for clean room demonstrations and testing. It creates a star schema with one fact table and four dimension tables, ensuring complete referential integrity before persisting to Unity Catalog.\n",
    "\n",
    "## Data Model\n",
    "\n",
    "**Fact Table:**\n",
    "* `visits` - Patient visit records linking to all dimension tables\n",
    "\n",
    "**Dimension Tables:**\n",
    "* `patients` - Patient demographic information\n",
    "* `doctors` - Healthcare provider information with specialties\n",
    "* `hospitals` - Hospital location data\n",
    "* `diagnoses` - Medical diagnosis codes (ICD-10 format)\n",
    "\n",
    "## Notebook Flow\n",
    "\n",
    "1. **Configure Catalog and Schema** - Widget inputs for target catalog and schema (default: `mkgs.clean_room_sample_data`)\n",
    "2. **Set Default Namespace** - Execute `USE CATALOG` and `USE SCHEMA` statements\n",
    "3. **Create Schema** - Ensure the target schema exists\n",
    "4. **Generate Sample Data** - Programmatically create randomized healthcare data:\n",
    "\t* Patients: 900-1100 (random)\n",
    "\t* Visits: 1400-1600 (random)\n",
    "\t* Doctors: 15-30 (random)\n",
    "\t* Hospitals: 3 (fixed)\n",
    "\t* Diagnoses: 15 (all available)\n",
    "5. **Verify Referential Integrity** - Run comprehensive checks:\n",
    "\t* Primary key uniqueness\n",
    "\t* Null value detection\n",
    "\t* Foreign key validation\n",
    "\t* **Only save tables if all checks pass**\n",
    "\n",
    "## Key Features\n",
    "\n",
    "* **Randomized data generation** - Different counts on each run (with seed for reproducibility)\n",
    "* **Referential integrity enforcement** - Tables only saved if validation succeeds\n",
    "* **Overwrite mode** - Tables are created or replaced without manual drops\n",
    "* **ICD-10 diagnosis codes** - Realistic medical coding standards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4aa06b64-ad88-4513-af9e-7ae006f0f289",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Configure Catalog and Schema"
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"catalog_use\", \"\", \"Catalog\")\n",
    "dbutils.widgets.text(\"schema_use\", \"\", \"Schema\")\n",
    "\n",
    "catalog = dbutils.widgets.get(\"catalog_use\")\n",
    "schema = dbutils.widgets.get(\"schema_use\")\n",
    "\n",
    "print(f\"\"\"\n",
    "catalog: {catalog}\n",
    "schema: {schema}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f470e77c-bfae-4926-9d83-6676413058ae",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Set Catalog and Schema"
    }
   },
   "outputs": [],
   "source": [
    "# Use the specified catalog and schema as defaults\n",
    "spark.sql(f\"USE CATALOG {catalog}\")\n",
    "spark.sql(f\"USE SCHEMA {schema}\")\n",
    "\n",
    "print(f\"Using catalog: {catalog}, schema: {schema}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af500892-6377-4366-9f76-20c311bc0066",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Show Current Catalog, Schema and Tables"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select current_catalog(), current_schema();\n",
    "    \n",
    "SHOW TABLES;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "676ee74b-c41a-4667-bd04-093586a6eb6e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Generate Sample Data"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "import random\n",
    "from datetime import datetime, timedelta, date\n",
    "from pyspark.sql.functions import to_date\n",
    "\n",
    "# Set seed for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "# Generate random counts for each entity type\n",
    "num_patients = random.randint(900, 1100)\n",
    "num_visits = random.randint(1400, 1600)\n",
    "num_doctors = random.randint(15, 30)\n",
    "num_hospitals = 3\n",
    "num_diagnoses = 15  # Use all 15 diagnoses\n",
    "\n",
    "print(f\"Generating data with:\")\n",
    "print(f\"  Patients: {num_patients}\")\n",
    "print(f\"  Visits: {num_visits}\")\n",
    "print(f\"  Doctors: {num_doctors}\")\n",
    "print(f\"  Hospitals: {num_hospitals}\")\n",
    "print(f\"  Diagnoses: {num_diagnoses}\")\n",
    "print()\n",
    "\n",
    "# Dimension table: hospitals (fixed at 3)\n",
    "hospitals = [\n",
    "\tRow(hospital_id=301, name=\"General Hospital\", city=\"Springfield\")\n",
    "\t, Row(hospital_id=302, name=\"City Medical Center\", city=\"Rivertown\")\n",
    "\t, Row(hospital_id=303, name=\"Children's Hospital\", city=\"Lakeside\")\n",
    "]\n",
    "df_hospitals = spark.createDataFrame(hospitals)\n",
    "\n",
    "# Dimension table: doctors (random between 15-30)\n",
    "specialties = [\n",
    "\t\"Cardiology\", \"Neurology\", \"Pediatrics\", \"Orthopedics\", \"Dermatology\"\n",
    "\t, \"Oncology\", \"Psychiatry\", \"Endocrinology\", \"Gastroenterology\", \"Pulmonology\"\n",
    "\t, \"Rheumatology\", \"Urology\", \"Nephrology\", \"Ophthalmology\", \"Otolaryngology\"\n",
    "\t, \"Radiology\", \"Anesthesiology\", \"Emergency Medicine\", \"Family Medicine\", \"Internal Medicine\"\n",
    "\t, \"Obstetrics\", \"Gynecology\", \"Pathology\", \"Surgery\", \"Hematology\"\n",
    "\t, \"Infectious Disease\", \"Allergy\", \"Sports Medicine\", \"Geriatrics\", \"Palliative Care\"\n",
    "]\n",
    "doctor_names = [\n",
    "\t\"Dr. Adams\", \"Dr. Baker\", \"Dr. Clark\", \"Dr. Davis\", \"Dr. Evans\"\n",
    "\t, \"Dr. Foster\", \"Dr. Green\", \"Dr. Harris\", \"Dr. Irwin\", \"Dr. Johnson\"\n",
    "\t, \"Dr. Kelly\", \"Dr. Lopez\", \"Dr. Murphy\", \"Dr. Nelson\", \"Dr. Owens\"\n",
    "\t, \"Dr. Parker\", \"Dr. Quinn\", \"Dr. Reed\", \"Dr. Stone\", \"Dr. Turner\"\n",
    "\t, \"Dr. Allen\", \"Dr. Bell\", \"Dr. Cooper\", \"Dr. Dixon\", \"Dr. Ellis\"\n",
    "\t, \"Dr. Fisher\", \"Dr. Gray\", \"Dr. Hughes\", \"Dr. Ingram\", \"Dr. Jenkins\"\n",
    "]\n",
    "\n",
    "doctors = []\n",
    "for i in range(num_doctors):\n",
    "\tdoctor_id = 201 + i\n",
    "\tname = doctor_names[i % len(doctor_names)]\n",
    "\tspecialty = specialties[i % len(specialties)]\n",
    "\tdoctors.append(Row(doctor_id=doctor_id, name=name, specialty=specialty))\n",
    "\n",
    "df_doctors = spark.createDataFrame(doctors)\n",
    "\n",
    "# Dimension table: diagnoses (all 15)\n",
    "all_diagnoses = [\n",
    "\tRow(diagnosis_id=401, code=\"I10\", description=\"Hypertension\")\n",
    "\t, Row(diagnosis_id=402, code=\"E11\", description=\"Type 2 Diabetes\")\n",
    "\t, Row(diagnosis_id=403, code=\"J45\", description=\"Asthma\")\n",
    "\t, Row(diagnosis_id=404, code=\"F32\", description=\"Depression\")\n",
    "\t, Row(diagnosis_id=405, code=\"M79\", description=\"Fibromyalgia\")\n",
    "\t, Row(diagnosis_id=406, code=\"K21\", description=\"GERD\")\n",
    "\t, Row(diagnosis_id=407, code=\"I25\", description=\"Coronary Artery Disease\")\n",
    "\t, Row(diagnosis_id=408, code=\"J44\", description=\"COPD\")\n",
    "\t, Row(diagnosis_id=409, code=\"N18\", description=\"Chronic Kidney Disease\")\n",
    "\t, Row(diagnosis_id=410, code=\"E78\", description=\"Hyperlipidemia\")\n",
    "\t, Row(diagnosis_id=411, code=\"M81\", description=\"Osteoporosis\")\n",
    "\t, Row(diagnosis_id=412, code=\"G43\", description=\"Migraine\")\n",
    "\t, Row(diagnosis_id=413, code=\"K58\", description=\"Irritable Bowel Syndrome\")\n",
    "\t, Row(diagnosis_id=414, code=\"L40\", description=\"Psoriasis\")\n",
    "\t, Row(diagnosis_id=415, code=\"F41\", description=\"Anxiety Disorder\")\n",
    "]\n",
    "\n",
    "# Use all 15 diagnoses\n",
    "diagnoses = all_diagnoses[:num_diagnoses]\n",
    "df_diagnoses = spark.createDataFrame(diagnoses)\n",
    "\n",
    "# Generate patients programmatically (random between 900-1100)\n",
    "first_names = [\"Alice\", \"Bob\", \"Carol\", \"David\", \"Emma\", \"Frank\", \"Grace\", \"Henry\", \"Iris\", \"Jack\"\n",
    "\t, \"Karen\", \"Leo\", \"Maria\", \"Nathan\", \"Olivia\", \"Paul\", \"Quinn\", \"Ryan\", \"Sarah\", \"Tom\"\n",
    "\t, \"Uma\", \"Victor\", \"Wendy\", \"Xavier\", \"Yara\", \"Zack\", \"Amy\", \"Brian\", \"Chloe\", \"Daniel\"\n",
    "\t, \"Emily\", \"Felix\", \"Gina\", \"Hugo\", \"Ivy\", \"James\", \"Kate\", \"Liam\", \"Mia\", \"Noah\"\n",
    "\t, \"Ava\", \"Ben\", \"Cara\", \"Dean\", \"Ella\", \"Finn\", \"Gia\", \"Hank\", \"Isla\", \"Jake\"]\n",
    "last_names = [\"Smith\", \"Jones\", \"Lee\", \"Kim\", \"Wilson\", \"Miller\", \"Taylor\", \"Brown\", \"Chen\", \"Davis\"\n",
    "\t, \"White\", \"Martinez\", \"Garcia\", \"Rodriguez\", \"Anderson\", \"Thomas\", \"Jackson\", \"Moore\", \"Martin\", \"Thompson\"\n",
    "\t, \"Patel\", \"Harris\", \"Clark\", \"Lewis\", \"Walker\", \"Hall\", \"Young\", \"King\", \"Wright\", \"Scott\"\n",
    "\t, \"Green\", \"Baker\", \"Adams\", \"Nelson\", \"Carter\", \"Mitchell\", \"Perez\", \"Roberts\", \"Turner\", \"Phillips\"]\n",
    "genders = [\"M\", \"F\"]\n",
    "\n",
    "patients = []\n",
    "for i in range(num_patients):\n",
    "\tpatient_id = 101 + i\n",
    "\tfirst_name = first_names[i % len(first_names)]\n",
    "\tlast_name = last_names[(i // len(first_names)) % len(last_names)]\n",
    "\tname = f\"{first_name} {last_name}\"\n",
    "\t# Generate random DOB between 1950 and 2010 as date object\n",
    "\tyear = 1950 + (i % 61)\n",
    "\tmonth = 1 + (i % 12)\n",
    "\tday = 1 + (i % 28)\n",
    "\tdob = date(year, month, day)\n",
    "\tgender = genders[i % 2]\n",
    "\tpatients.append(Row(patient_id=patient_id, name=name, dob=dob, gender=gender))\n",
    "\n",
    "df_patients = spark.createDataFrame(patients)\n",
    "\n",
    "# Generate visits programmatically with valid foreign keys (random between 1400-1600)\n",
    "patient_ids = list(range(101, 101 + num_patients))\n",
    "doctor_ids = list(range(201, 201 + num_doctors))\n",
    "hospital_ids = [301, 302, 303]\n",
    "diagnosis_ids = [d.diagnosis_id for d in diagnoses]\n",
    "\n",
    "patient_visits = []\n",
    "start_date = date(2025, 1, 1)\n",
    "for i in range(num_visits):\n",
    "\tvisit_id = i + 1\n",
    "\tpatient_id = random.choice(patient_ids)\n",
    "\tdoctor_id = random.choice(doctor_ids)\n",
    "\thospital_id = random.choice(hospital_ids)\n",
    "\tdiagnosis_id = random.choice(diagnosis_ids)\n",
    "\t# Spread visits across 365 days as date object\n",
    "\tvisit_date = start_date + timedelta(days=i % 365)\n",
    "\tpatient_visits.append(Row(\n",
    "\t\tvisit_id=visit_id\n",
    "\t\t, patient_id=patient_id\n",
    "\t\t, doctor_id=doctor_id\n",
    "\t\t, hospital_id=hospital_id\n",
    "\t\t, diagnosis_id=diagnosis_id\n",
    "\t\t, visit_date=visit_date\n",
    "\t))\n",
    "\n",
    "df_patient_visits = spark.createDataFrame(patient_visits)\n",
    "\n",
    "print(f\"\\nGenerated:\")\n",
    "print(f\"  {df_patients.count()} patients\")\n",
    "print(f\"  {df_patient_visits.count()} visits\")\n",
    "print(f\"  {df_doctors.count()} doctors\")\n",
    "print(f\"  {df_hospitals.count()} hospitals\")\n",
    "print(f\"  {df_diagnoses.count()} diagnoses\")\n",
    "\n",
    "display(df_patient_visits.limit(10))\n",
    "display(df_patients.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b4c57bc-13d5-4f56-b932-6a1642ea3910",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Verify Referential Integrity"
    }
   },
   "outputs": [],
   "source": [
    "print(\"=== REFERENTIAL INTEGRITY CHECKS ===\\n\")\n",
    "\n",
    "# Check 1: Verify primary key uniqueness in dimension tables\n",
    "print(\"1. PRIMARY KEY UNIQUENESS:\")\n",
    "print(f\"   Patients: {df_patients.count()} total, {df_patients.select('patient_id').distinct().count()} unique patient_ids\")\n",
    "print(f\"   Doctors: {df_doctors.count()} total, {df_doctors.select('doctor_id').distinct().count()} unique doctor_ids\")\n",
    "print(f\"   Hospitals: {df_hospitals.count()} total, {df_hospitals.select('hospital_id').distinct().count()} unique hospital_ids\")\n",
    "print(f\"   Diagnoses: {df_diagnoses.count()} total, {df_diagnoses.select('diagnosis_id').distinct().count()} unique diagnosis_ids\")\n",
    "print(f\"   Visits: {df_patient_visits.count()} total, {df_patient_visits.select('visit_id').distinct().count()} unique visit_ids\")\n",
    "\n",
    "# Check 2: Verify no nulls in key columns\n",
    "print(\"\\n2. NULL CHECKS IN KEY COLUMNS:\")\n",
    "print(f\"   Null patient_ids in visits: {df_patient_visits.filter('patient_id IS NULL').count()}\")\n",
    "print(f\"   Null doctor_ids in visits: {df_patient_visits.filter('doctor_id IS NULL').count()}\")\n",
    "print(f\"   Null hospital_ids in visits: {df_patient_visits.filter('hospital_id IS NULL').count()}\")\n",
    "print(f\"   Null diagnosis_ids in visits: {df_patient_visits.filter('diagnosis_id IS NULL').count()}\")\n",
    "\n",
    "# Check 3: Verify foreign key references (patient_id)\n",
    "print(\"\\n3. FOREIGN KEY INTEGRITY:\")\n",
    "patient_ids_in_visits = df_patient_visits.select('patient_id').distinct()\n",
    "patient_ids_in_dim = df_patients.select('patient_id')\n",
    "invalid_patients = patient_ids_in_visits.join(patient_ids_in_dim, 'patient_id', 'left_anti')\n",
    "print(f\"   Invalid patient_ids in visits: {invalid_patients.count()}\")\n",
    "if invalid_patients.count() > 0:\n",
    "\tprint(f\"   Invalid patient_ids: {[row.patient_id for row in invalid_patients.collect()]}\")\n",
    "\n",
    "# Check 4: Verify foreign key references (doctor_id)\n",
    "doctor_ids_in_visits = df_patient_visits.select('doctor_id').distinct()\n",
    "doctor_ids_in_dim = df_doctors.select('doctor_id')\n",
    "invalid_doctors = doctor_ids_in_visits.join(doctor_ids_in_dim, 'doctor_id', 'left_anti')\n",
    "print(f\"   Invalid doctor_ids in visits: {invalid_doctors.count()}\")\n",
    "if invalid_doctors.count() > 0:\n",
    "\tprint(f\"   Invalid doctor_ids: {[row.doctor_id for row in invalid_doctors.collect()]}\")\n",
    "\n",
    "# Check 5: Verify foreign key references (hospital_id)\n",
    "hospital_ids_in_visits = df_patient_visits.select('hospital_id').distinct()\n",
    "hospital_ids_in_dim = df_hospitals.select('hospital_id')\n",
    "invalid_hospitals = hospital_ids_in_visits.join(hospital_ids_in_dim, 'hospital_id', 'left_anti')\n",
    "print(f\"   Invalid hospital_ids in visits: {invalid_hospitals.count()}\")\n",
    "if invalid_hospitals.count() > 0:\n",
    "\tprint(f\"   Invalid hospital_ids: {[row.hospital_id for row in invalid_hospitals.collect()]}\")\n",
    "\n",
    "# Check 6: Verify foreign key references (diagnosis_id)\n",
    "diagnosis_ids_in_visits = df_patient_visits.select('diagnosis_id').distinct()\n",
    "diagnosis_ids_in_dim = df_diagnoses.select('diagnosis_id')\n",
    "invalid_diagnoses = diagnosis_ids_in_visits.join(diagnosis_ids_in_dim, 'diagnosis_id', 'left_anti')\n",
    "print(f\"   Invalid diagnosis_ids in visits: {invalid_diagnoses.count()}\")\n",
    "if invalid_diagnoses.count() > 0:\n",
    "\tprint(f\"   Invalid diagnosis_ids: {[row.diagnosis_id for row in invalid_diagnoses.collect()]}\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n=== SUMMARY ===\")\n",
    "total_issues = (\n",
    "\t(df_patients.count() - df_patients.select('patient_id').distinct().count()) +\n",
    "\t(df_doctors.count() - df_doctors.select('doctor_id').distinct().count()) +\n",
    "\t(df_hospitals.count() - df_hospitals.select('hospital_id').distinct().count()) +\n",
    "\t(df_diagnoses.count() - df_diagnoses.select('diagnosis_id').distinct().count()) +\n",
    "\t(df_patient_visits.count() - df_patient_visits.select('visit_id').distinct().count()) +\n",
    "\tdf_patient_visits.filter('patient_id IS NULL').count() +\n",
    "\tdf_patient_visits.filter('doctor_id IS NULL').count() +\n",
    "\tdf_patient_visits.filter('hospital_id IS NULL').count() +\n",
    "\tdf_patient_visits.filter('diagnosis_id IS NULL').count() +\n",
    "\tinvalid_patients.count() +\n",
    "\tinvalid_doctors.count() +\n",
    "\tinvalid_hospitals.count() +\n",
    "\tinvalid_diagnoses.count()\n",
    ")\n",
    "\n",
    "if total_issues == 0:\n",
    "\tprint(\"✓ All referential integrity checks PASSED\")\n",
    "\tprint(\"✓ No duplicate primary keys\")\n",
    "\tprint(\"✓ No null foreign keys\")\n",
    "\tprint(\"✓ All foreign keys reference valid dimension records\")\n",
    "\t\n",
    "\t# Only save tables if integrity checks pass\n",
    "\tprint(\"\\n=== SAVING TABLES ===\")\n",
    "\tdf_patient_visits.write.mode(\"overwrite\").saveAsTable(\"visits\")\n",
    "\tprint(\"✓ Saved table: visits\")\n",
    "\t\n",
    "\tdf_patients.write.mode(\"overwrite\").saveAsTable(\"patients\")\n",
    "\tprint(\"✓ Saved table: patients\")\n",
    "\t\n",
    "\tdf_doctors.write.mode(\"overwrite\").saveAsTable(\"doctors\")\n",
    "\tprint(\"✓ Saved table: doctors\")\n",
    "\t\n",
    "\tdf_hospitals.write.mode(\"overwrite\").saveAsTable(\"hospitals\")\n",
    "\tprint(\"✓ Saved table: hospitals\")\n",
    "\t\n",
    "\tdf_diagnoses.write.mode(\"overwrite\").saveAsTable(\"diagnoses\")\n",
    "\tprint(\"✓ Saved table: diagnoses\")\n",
    "\t\n",
    "\tprint(\"\\n=== APPLYING DELTA TABLE PROPERTIES ===\")\n",
    "\t\n",
    "\t# Apply liquid clustering (auto mode) - visits is fact table with multiple join keys\n",
    "\tspark.sql(\"ALTER TABLE visits CLUSTER BY (visit_date, patient_id, doctor_id, hospital_id, diagnosis_id)\")\n",
    "\tprint(\"✓ Applied liquid clustering to visits table\")\n",
    "\t\n",
    "\t# Apply liquid clustering to dimension tables by their primary keys\n",
    "\tspark.sql(\"ALTER TABLE patients CLUSTER BY (patient_id)\")\n",
    "\tprint(\"✓ Applied liquid clustering to patients table\")\n",
    "\t\n",
    "\tspark.sql(\"ALTER TABLE doctors CLUSTER BY (doctor_id)\")\n",
    "\tprint(\"✓ Applied liquid clustering to doctors table\")\n",
    "\t\n",
    "\tspark.sql(\"ALTER TABLE hospitals CLUSTER BY (hospital_id)\")\n",
    "\tprint(\"✓ Applied liquid clustering to hospitals table\")\n",
    "\t\n",
    "\tspark.sql(\"ALTER TABLE diagnoses CLUSTER BY (diagnosis_id)\")\n",
    "\tprint(\"✓ Applied liquid clustering to diagnoses table\")\n",
    "\t\n",
    "\t# Enable Change Data Feed on all tables\n",
    "\tfor table in ['visits', 'patients', 'doctors', 'hospitals', 'diagnoses']:\n",
    "\t\tspark.sql(f\"ALTER TABLE {table} SET TBLPROPERTIES (delta.enableChangeDataFeed = true)\")\n",
    "\tprint(\"✓ Enabled Change Data Feed on all tables\")\n",
    "\t\n",
    "\t# Enable auto compaction and optimized writes\n",
    "\tfor table in ['visits', 'patients', 'doctors', 'hospitals', 'diagnoses']:\n",
    "\t\tspark.sql(f\"ALTER TABLE {table} SET TBLPROPERTIES (delta.autoOptimize.autoCompact = true)\")\n",
    "\t\tspark.sql(f\"ALTER TABLE {table} SET TBLPROPERTIES (delta.autoOptimize.optimizeWrite = true)\")\n",
    "\tprint(\"✓ Enabled auto compaction and optimized writes on all tables\")\n",
    "\t\n",
    "\t# Enable deletion vectors for efficient deletes/updates\n",
    "\tfor table in ['visits', 'patients', 'doctors', 'hospitals', 'diagnoses']:\n",
    "\t\tspark.sql(f\"ALTER TABLE {table} SET TBLPROPERTIES (delta.enableDeletionVectors = true)\")\n",
    "\tprint(\"✓ Enabled deletion vectors on all tables\")\n",
    "\t\n",
    "\t# Enable predictive optimization (auto-optimize)\n",
    "\tfor table in ['visits', 'patients', 'doctors', 'hospitals', 'diagnoses']:\n",
    "\t\tspark.sql(f\"ALTER TABLE {table} SET TBLPROPERTIES (delta.autoOptimize.autoCompact = true, delta.targetFileSize = '128MB')\")\n",
    "\tprint(\"✓ Set target file size to 128MB for optimal performance\")\n",
    "\t\n",
    "\tprint(\"\\n✓ All tables created/replaced successfully with Delta optimizations\")\n",
    "else:\n",
    "\tprint(f\"✗ Found {total_issues} integrity issue(s)\")\n",
    "\tprint(\"✗ TABLES NOT SAVED - Fix integrity issues before saving\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2932d12-d34d-47e0-a4ba-e1c1a5803dab",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Add Table and Column Comments"
    }
   },
   "outputs": [],
   "source": [
    "print(\"=== ADDING TABLE AND COLUMN COMMENTS ===\")\n",
    "\n",
    "# Add comments to patients table\n",
    "spark.sql(\"\"\"\n",
    "\tALTER TABLE patients\n",
    "\tSET TBLPROPERTIES ('comment' = 'Patient demographic information including name, date of birth, and gender')\n",
    "\"\"\")\n",
    "spark.sql(\"ALTER TABLE patients ALTER COLUMN patient_id COMMENT 'Unique identifier for each patient'\")\n",
    "spark.sql(\"ALTER TABLE patients ALTER COLUMN name COMMENT 'Full name of the patient'\")\n",
    "spark.sql(\"ALTER TABLE patients ALTER COLUMN dob COMMENT 'Date of birth in YYYY-MM-DD format'\")\n",
    "spark.sql(\"ALTER TABLE patients ALTER COLUMN gender COMMENT 'Gender (M/F)'\")\n",
    "print(\"✓ Added comments to patients table\")\n",
    "\n",
    "# Add comments to doctors table\n",
    "spark.sql(\"\"\"\n",
    "\tALTER TABLE doctors\n",
    "\tSET TBLPROPERTIES ('comment' = 'Healthcare provider information with medical specialties')\n",
    "\"\"\")\n",
    "spark.sql(\"ALTER TABLE doctors ALTER COLUMN doctor_id COMMENT 'Unique identifier for each doctor'\")\n",
    "spark.sql(\"ALTER TABLE doctors ALTER COLUMN name COMMENT 'Full name of the doctor'\")\n",
    "spark.sql(\"ALTER TABLE doctors ALTER COLUMN specialty COMMENT 'Medical specialty or area of practice'\")\n",
    "print(\"✓ Added comments to doctors table\")\n",
    "\n",
    "# Add comments to hospitals table\n",
    "spark.sql(\"\"\"\n",
    "\tALTER TABLE hospitals\n",
    "\tSET TBLPROPERTIES ('comment' = 'Hospital location and facility information')\n",
    "\"\"\")\n",
    "spark.sql(\"ALTER TABLE hospitals ALTER COLUMN hospital_id COMMENT 'Unique identifier for each hospital'\")\n",
    "spark.sql(\"ALTER TABLE hospitals ALTER COLUMN name COMMENT 'Name of the hospital facility'\")\n",
    "spark.sql(\"ALTER TABLE hospitals ALTER COLUMN city COMMENT 'City where the hospital is located'\")\n",
    "print(\"✓ Added comments to hospitals table\")\n",
    "\n",
    "# Add comments to diagnoses table\n",
    "spark.sql(\"\"\"\n",
    "\tALTER TABLE diagnoses\n",
    "\tSET TBLPROPERTIES ('comment' = 'Medical diagnosis codes following ICD-10 standards')\n",
    "\"\"\")\n",
    "spark.sql(\"ALTER TABLE diagnoses ALTER COLUMN diagnosis_id COMMENT 'Unique identifier for each diagnosis'\")\n",
    "spark.sql(\"ALTER TABLE diagnoses ALTER COLUMN code COMMENT 'ICD-10 diagnosis code'\")\n",
    "spark.sql(\"ALTER TABLE diagnoses ALTER COLUMN description COMMENT 'Human-readable description of the diagnosis'\")\n",
    "print(\"✓ Added comments to diagnoses table\")\n",
    "\n",
    "# Add comments to visits table\n",
    "spark.sql(\"\"\"\n",
    "\tALTER TABLE visits\n",
    "\tSET TBLPROPERTIES ('comment' = 'Patient visit records linking patients, doctors, hospitals, and diagnoses')\n",
    "\"\"\")\n",
    "spark.sql(\"ALTER TABLE visits ALTER COLUMN visit_id COMMENT 'Unique identifier for each visit'\")\n",
    "spark.sql(\"ALTER TABLE visits ALTER COLUMN patient_id COMMENT 'Foreign key reference to patients table'\")\n",
    "spark.sql(\"ALTER TABLE visits ALTER COLUMN doctor_id COMMENT 'Foreign key reference to doctors table'\")\n",
    "spark.sql(\"ALTER TABLE visits ALTER COLUMN hospital_id COMMENT 'Foreign key reference to hospitals table'\")\n",
    "spark.sql(\"ALTER TABLE visits ALTER COLUMN diagnosis_id COMMENT 'Foreign key reference to diagnoses table'\")\n",
    "spark.sql(\"ALTER TABLE visits ALTER COLUMN visit_date COMMENT 'Date of the visit in YYYY-MM-DD format'\")\n",
    "print(\"✓ Added comments to visits table\")\n",
    "\n",
    "print(\"\\n✓ All table and column comments added successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e070879-ab47-4efd-8330-d2272fd1bbd5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Add Primary and Foreign Key Constraints"
    }
   },
   "outputs": [],
   "source": [
    "print(\"=== SETTING PRIMARY KEY COLUMNS TO NOT NULL ===\")\n",
    "\n",
    "# Set primary key columns to NOT NULL\n",
    "spark.sql(\"ALTER TABLE patients ALTER COLUMN patient_id SET NOT NULL\")\n",
    "print(\"✓ Set patients.patient_id to NOT NULL\")\n",
    "\n",
    "spark.sql(\"ALTER TABLE doctors ALTER COLUMN doctor_id SET NOT NULL\")\n",
    "print(\"✓ Set doctors.doctor_id to NOT NULL\")\n",
    "\n",
    "spark.sql(\"ALTER TABLE hospitals ALTER COLUMN hospital_id SET NOT NULL\")\n",
    "print(\"✓ Set hospitals.hospital_id to NOT NULL\")\n",
    "\n",
    "spark.sql(\"ALTER TABLE diagnoses ALTER COLUMN diagnosis_id SET NOT NULL\")\n",
    "print(\"✓ Set diagnoses.diagnosis_id to NOT NULL\")\n",
    "\n",
    "spark.sql(\"ALTER TABLE visits ALTER COLUMN visit_id SET NOT NULL\")\n",
    "print(\"✓ Set visits.visit_id to NOT NULL\")\n",
    "\n",
    "print(\"\\n=== ADDING PRIMARY AND FOREIGN KEY CONSTRAINTS ===\")\n",
    "\n",
    "# Add primary key constraints with RELY\n",
    "spark.sql(\"ALTER TABLE patients DROP CONSTRAINT IF EXISTS patients_pk CASCADE\")\n",
    "spark.sql(\"ALTER TABLE patients ADD CONSTRAINT patients_pk PRIMARY KEY (patient_id) RELY\")\n",
    "print(\"✓ Added primary key constraint to patients table\")\n",
    "\n",
    "spark.sql(\"ALTER TABLE doctors DROP CONSTRAINT IF EXISTS doctors_pk CASCADE\")\n",
    "spark.sql(\"ALTER TABLE doctors ADD CONSTRAINT doctors_pk PRIMARY KEY (doctor_id) RELY\")\n",
    "print(\"✓ Added primary key constraint to doctors table\")\n",
    "\n",
    "spark.sql(\"ALTER TABLE hospitals DROP CONSTRAINT IF EXISTS hospitals_pk CASCADE\")\n",
    "spark.sql(\"ALTER TABLE hospitals ADD CONSTRAINT hospitals_pk PRIMARY KEY (hospital_id) RELY\")\n",
    "print(\"✓ Added primary key constraint to hospitals table\")\n",
    "\n",
    "spark.sql(\"ALTER TABLE diagnoses DROP CONSTRAINT IF EXISTS diagnoses_pk CASCADE\")\n",
    "spark.sql(\"ALTER TABLE diagnoses ADD CONSTRAINT diagnoses_pk PRIMARY KEY (diagnosis_id) RELY\")\n",
    "print(\"✓ Added primary key constraint to diagnoses table\")\n",
    "\n",
    "spark.sql(\"ALTER TABLE visits DROP CONSTRAINT IF EXISTS visits_pk CASCADE\")\n",
    "spark.sql(\"ALTER TABLE visits ADD CONSTRAINT visits_pk PRIMARY KEY (visit_id) RELY\")\n",
    "print(\"✓ Added primary key constraint to visits table\")\n",
    "\n",
    "# Add foreign key constraints to visits table with RELY using full three-level namespace\n",
    "spark.sql(\"ALTER TABLE visits DROP CONSTRAINT IF EXISTS visits_patient_fk CASCADE\")\n",
    "spark.sql(f\"\"\"\n",
    "\tALTER TABLE visits\n",
    "\tADD CONSTRAINT visits_patient_fk\n",
    "\tFOREIGN KEY (patient_id) REFERENCES {catalog}.{schema}.patients(patient_id) RELY\n",
    "\"\"\")\n",
    "print(f\"✓ Added foreign key constraint: visits.patient_id -> {catalog}.{schema}.patients.patient_id\")\n",
    "\n",
    "spark.sql(\"ALTER TABLE visits DROP CONSTRAINT IF EXISTS visits_doctor_fk CASCADE\")\n",
    "spark.sql(f\"\"\"\n",
    "\tALTER TABLE visits\n",
    "\tADD CONSTRAINT visits_doctor_fk\n",
    "\tFOREIGN KEY (doctor_id) REFERENCES {catalog}.{schema}.doctors(doctor_id) RELY\n",
    "\"\"\")\n",
    "print(f\"✓ Added foreign key constraint: visits.doctor_id -> {catalog}.{schema}.doctors.doctor_id\")\n",
    "\n",
    "spark.sql(\"ALTER TABLE visits DROP CONSTRAINT IF EXISTS visits_hospital_fk CASCADE\")\n",
    "spark.sql(f\"\"\"\n",
    "\tALTER TABLE visits\n",
    "\tADD CONSTRAINT visits_hospital_fk\n",
    "\tFOREIGN KEY (hospital_id) REFERENCES {catalog}.{schema}.hospitals(hospital_id) RELY\n",
    "\"\"\")\n",
    "print(f\"✓ Added foreign key constraint: visits.hospital_id -> {catalog}.{schema}.hospitals.hospital_id\")\n",
    "\n",
    "spark.sql(\"ALTER TABLE visits DROP CONSTRAINT IF EXISTS visits_diagnosis_fk CASCADE\")\n",
    "spark.sql(f\"\"\"\n",
    "\tALTER TABLE visits\n",
    "\tADD CONSTRAINT visits_diagnosis_fk\n",
    "\tFOREIGN KEY (diagnosis_id) REFERENCES {catalog}.{schema}.diagnoses(diagnosis_id) RELY\n",
    "\"\"\")\n",
    "print(f\"✓ Added foreign key constraint: visits.diagnosis_id -> {catalog}.{schema}.diagnoses.diagnosis_id\")\n",
    "\n",
    "print(\"\\n✓ All primary and foreign key constraints added successfully with RELY option\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8285984814210484,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "create sample data",
   "widgets": {
    "catalog_use": {
     "currentValue": "mkgs_dev",
     "nuid": "5a5015bd-0c7b-4e79-918a-2d4624c44622",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Catalog",
      "name": "catalog_use",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "Catalog",
      "name": "catalog_use",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "schema_use": {
     "currentValue": "dev_matthew_giglia_clean_room_sample_data",
     "nuid": "5022212b-f608-40c5-b87b-3f6885af3ab7",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Schema",
      "name": "schema_use",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "Schema",
      "name": "schema_use",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
